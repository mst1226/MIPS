<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>root1 &#8211; MIPS</title>
	<atom:link href="http://new2/author/root1/feed/?simply_static_page=2066" rel="self" type="application/rss+xml" />
	<link>/</link>
	<description>Multimodal Intelligent Perceiving System</description>
	<lastBuildDate>Wed, 17 May 2023 07:09:55 +0000</lastBuildDate>
	<language>zh-CN</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	<generator>https://wordpress.org/?v=6.4.1</generator>

<image>
	<url>/wp-content/uploads/2023/05/微信图片_20230419161532-150x150.png</url>
	<title>root1 &#8211; MIPS</title>
	<link>/</link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>你好</title>
		<link>/2023/05/17/%e4%bd%a0%e5%a5%bd/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Wed, 17 May 2023 07:05:31 +0000</pubDate>
				<category><![CDATA[未分类]]></category>
		<guid isPermaLink="false">/?p=731</guid>

					<description><![CDATA[]]></description>
										<content:encoded><![CDATA[]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>当小语种遇上大模型：自监督预训练模型显著提升低资源小语种语音识别性能</title>
		<link>/2023/05/16/%e5%bd%93%e5%b0%8f%e8%af%ad%e7%a7%8d%e9%81%87%e4%b8%8a%e5%a4%a7%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%87%aa%e7%9b%91%e7%9d%a3%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e6%98%be%e8%91%97%e6%8f%90%e5%8d%87/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Tue, 16 May 2023 11:27:24 +0000</pubDate>
				<category><![CDATA[project]]></category>
		<guid isPermaLink="false">/?p=593</guid>

					<description><![CDATA[在大规模数据和算力的支撑下，语音领域自监督预学习取得了突破性进展，预训练模型则凭借其优异的性能而备受关注。本文 ... <a title="当小语种遇上大模型：自监督预训练模型显著提升低资源小语种语音识别性能" class="read-more" href="/2023/05/16/%e5%bd%93%e5%b0%8f%e8%af%ad%e7%a7%8d%e9%81%87%e4%b8%8a%e5%a4%a7%e6%a8%a1%e5%9e%8b%ef%bc%9a%e8%87%aa%e7%9b%91%e7%9d%a3%e9%a2%84%e8%ae%ad%e7%bb%83%e6%a8%a1%e5%9e%8b%e6%98%be%e8%91%97%e6%8f%90%e5%8d%87/" aria-label="继续阅读当小语种遇上大模型：自监督预训练模型显著提升低资源小语种语音识别性能">阅读更多</a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/yMZb2azAvUJz.jpg" alt=""/></figure>



<p><em>在大规模数据和算力的支撑下，语音领域自监督预学习取得了突破性进展，预训练模型则凭借其优异的性能而备受关注</em><em>。本文将一系列大规模预训练模型进行了深入对比，针对低资源小语种自动语音识别这一下游任务展开探索，并从模型、方法、语种等多个角度全面分析并总结了性能差异和内部机理。日前，该工作已被IEEE信号处理协会顶级期刊《IEEE Journal of Selected Topics in Signal Processing》接收。</em></p>



<p>近期语音自监督学习凭借其在多个下游任务中的出色性能而备受关注，这为低资源语种语音识别带来了新的发展空间。本文中，我们对比分析了<strong>多个</strong><strong>wav2vec</strong><strong>系列预训练模型在</strong><strong>15</strong><strong>个不同的低资源小语种上的表现。研究内容主要针对模型预训练阶段的两个重要因素（模型框架和训练数据）、三种微调方法以及预训练模型表征应用展开</strong><strong>。</strong>首先，我们研究了使用不同语音数据或自监督架构（wav2vec2.0、HuBERT和WavLM）的预训练模型在低资源语种中的语音识别性能。其次，我们探索了语音数据利用、多语言学习和音素识别任务在微调阶段的应用。此外，我们还借助相似性分析探讨了模型微调对不同Transformer层预训练表征的影响，涉及不同的预训练体系结构和15个语种。同时我们将预训练表征应用于端到端和传统混合系统，以验证前文的表征分析，并取得了更优的性能。</p>



<p>本文使用的低资源语种数据基本情况如下表所示。共&nbsp;15&nbsp;个语种，涵盖&nbsp;11&nbsp;个不同的语系。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/09/NFZbeijM7nEb.png" alt="640.png" title="640.png"/></figure>



<p>本文使用的15个语种的基本情况</p>



<p><strong>一、预训练阶段影响因素对比</strong></p>



<p>基于预训练模型的语音识别系统性能与预训练音频数据密不可分，不同的预训练架构也会带来性能差异。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/uqUfyq36Rbqy.png" alt="2.png" title="remote.png"/></figure>



<p>预训练阶段影响因素（预训练数据、预训练架构）</p>



<p>我们比较了一些现有的开源预训练模型，以&nbsp;<strong>探索不同预训练数据集和预训练架构对低资源小语种语音识别性能的影响</strong>&nbsp;，主要模型的基本情况如下表所示。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/FBzEV3fYf6z2.png" alt="3.png" title="remote.png"/></figure>



<p><strong>1.1&nbsp;</strong><strong>预训练数据</strong><strong></strong></p>



<p>为保证实验的公平性，在对比不同数据集训练得到的预训练模型时，我们选择了四个wav2vec2.0模型，包括一个英语单语种模型和三个多语种模型。</p>



<p>对于&nbsp;OpenASR21&nbsp;数据集中的&nbsp;15&nbsp;种语言，我们从每个语系中选择一个语种作为代表，分别使用&nbsp;10h标注&nbsp;数据微调&nbsp;wav2vec2.0&nbsp;预训练模型。标注文本建模单元为字母或字素，并额外加入词分隔符。实验结果如下表所示。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/QNbYf2qqe2am.png" alt="remote.png" title="remote.png"/></figure>



<p>通过表中结果可以发现，&nbsp;XLSR-128&nbsp;预训练模型在各个语种中均取得最低词错误率。在三个&nbsp;LARGE&nbsp;模型中，用英语音频训练得到的&nbsp;w2v-EN-60k&nbsp;模型明显处于劣势。在&nbsp;BASE&nbsp;模型和&nbsp;LARGE&nbsp;型模型之间的比较中，多语言&nbsp;BASE&nbsp;模型&nbsp;CLSRIL-23&nbsp;相比于英语&nbsp;LARGE&nbsp;模型&nbsp;w2v-EN-60k&nbsp;虽然使用的总训练音频量更少，但在&nbsp;10&nbsp;个语种中的&nbsp;9&nbsp;个语种上优于后者，以上结果说明了<strong>在小语种语音识别任务上，语音数据分布情况（多语种）相比于总数据量更为重要；使用更多语种更大数据量训练得到的模型性能相对更优。</strong></p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/i6vMJjamUJNj.png" alt="remote.png" title="remote.png"/></figure>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/Zn6zqaNziEbm.png" alt="remote.png" title="remote.png"/></figure>



<p>不同预训练模型在微调前后各层输出表征的相似度</p>



<p>为了探究微调前后模型各层参数的变化情况，我们采用CKA方法对预训练模型不同位置Transformer的输出表征进行了相似度分析，如上图所示，坐标轴数字为Transformer层的索引。可以发现<strong>各个预训练模型最后几层的输出表征在微调前后都发生了较明显变化，这表明在靠近输出端的层中可能蕴含更多的语义或文本相关的高层信息，与下游任务、语种特性紧密相关。相反地，在微调后，靠近输入端的前几层表征保留了更多的原预训练模型信息，同时前几层输出表征之间的相似性较高，这也表明这些层中可能存在一定的冗余部分。</strong>由于我们更关注对应层的表征在微调后的变化情况，因此将热度图中的左下角到右上角这一对角线的情况借助折线图展现出来。可以发现不同语种之间的表征变化情况差异较小，而BASE模型和LARGE模型的变化趋势存在较明显差异。</p>



<p>进一步，我们将各个语种得到的相似度取平均值后展示在下图中，以更加直观地对比不同模型之间的差异。对比三个&nbsp;LARGE&nbsp;模型（&nbsp;w2v-EN-60k&nbsp;、&nbsp;XLSR-53&nbsp;、&nbsp;XLSR-128&nbsp;），<strong>我们发现模型在最后几层的相似度大小与模型的语音识别性能之间可能存在一定联系，即折线越靠上，说明对应表征的相似性越高、微调前后参数变化小，则对应的识别结果越好。</strong><strong></strong></p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/6z6Fbqf2eEJ3.png" alt="remote.png" title="remote.png"/></figure>



<p><strong>1.2&nbsp;</strong><strong>预训练架构：</strong><strong>wav2vec2.0, HuBERT</strong><strong>和</strong><strong>WavLM</strong><strong></strong></p>



<p>为了保证公平性，我们选择了三个使用相近预训练语料库和模型参数的开源预训练模型w2v-EN-60k、HuBERT-EN-60k和WavLM-EN-94k作为研究对象。其中w2v-EN-60k和HuBERT-EN-60k通过Libri-light数据集的6万小时英语数据训练得到。WavLM-EN-94k则额外使用了1万小时Gigaspeech和2.4万小时VoxPopulil数据集进行模型预训练。</p>



<p>实验包括两组系统设置，分别为&nbsp;FT&nbsp;和&nbsp;FE&nbsp;，其中&nbsp;FT&nbsp;指的是直接通过&nbsp;CTC&nbsp;准则进行模型微调，&nbsp;FE&nbsp;则表示将预训练模型作为前端特征提取模块用于&nbsp;CTC/Attention&nbsp;混合端到端系统。对应&nbsp;15&nbsp;个语种的语音识别结果如下表所示。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/7fAb2mvYfqQv.png" alt="remote.png" title="remote.png"/></figure>



<p>通过上表中结果我们可以发现&nbsp;WavLM-EN-94k&nbsp;模型在&nbsp;FT&nbsp;和&nbsp;FE&nbsp;系统中的&nbsp;15&nbsp;个语种的词错误率均取得了最低值。在微调&nbsp;FT&nbsp;系统中，&nbsp;<strong>WavLM-EN-94k</strong>&nbsp;<strong>预训练模型与</strong>&nbsp;<strong>w2v-EN-60k</strong>&nbsp;<strong>和</strong>&nbsp;<strong>HuBERT-EN-60k</strong>&nbsp;<strong>相比，平均词错误率分别降低了</strong>&nbsp;<strong>18.7%</strong>&nbsp;<strong>和</strong>&nbsp;<strong>12.3%</strong>&nbsp;<strong>，这也说明了在低资源小语种语音识别任务上</strong>&nbsp;<strong>WavLM</strong>&nbsp;<strong>相比于</strong>&nbsp;<strong>wav2vec2.0</strong>&nbsp;<strong>和</strong>&nbsp;<strong>HuBERT</strong>&nbsp;<strong>模型具有明显优越性</strong>&nbsp;<strong>。</strong>&nbsp;同样地，我们把以上微调之后的三个预训练模型两两之间的各层表征之间的相似度情况展示如下。<strong>可以发现模型之间的主要差别集中最后三层，</strong>&nbsp;<strong>HuBERT</strong>&nbsp;<strong>和</strong>&nbsp;<strong>WavLM</strong>&nbsp;<strong>两者之间相似度更高，随着模型层数增加，不同语种之间的差异也逐渐拉大。</strong><strong></strong></p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/ZRzyUfVvaUfa.png" alt="remote.png" title="remote.png"/></figure>



<p>同时我们发现wav2vec2.0和HuBERT模型在不同系统和不同语种上的表现存在一定区别。直接微调时，使用HuBERT模型在gn、jv、ku、so、sw和tl这6种语言上的词错误率更低，其余语种词错误率则高于wav2vec2.0模型，且有较大差距；而在FE系统中，HuBERT模型在所有15个语种中均表现更优。因此<strong>我们推测</strong><strong>wav2vec2.0</strong><strong>和</strong><strong>HuBERT</strong><strong>模型对语种或某些语言特征存在偏好。我们通过调查各个语种的基本信息发现，</strong><strong>6</strong><strong>个在</strong><strong>HuBERT</strong><strong>模型表现更优的语种均使用拉丁书写体系，而其余</strong><strong>10</strong><strong>种语言除越南语外没有拉丁书写系统。从音素集角度来看，前面提到的</strong><strong>6</strong><strong>个语种平均音素数量较少，约占其余</strong><strong>10</strong><strong>个语种的</strong><strong>80%</strong><strong>。</strong><strong></strong></p>



<p>为了全面对比三个预训练模型微调前后的变化情况，并涵盖不同语种的差异情况，我们进行了&nbsp;CKA&nbsp;相似性分析将结果展示在下图中，其中相同颜色的多条曲线对应使用同一预训练模型的不同语种。可以看到&nbsp;wav2vec2.0&nbsp;模型相比于其他模型对于微调的变化更加明显。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/jmm2U3vyYfeq.png" alt="remote.png" title="remote.png"/></figure>



<p><strong>二、微调策略</strong></p>



<p>模型微调是将预训练模型用于目标下游任务的一种快捷有效方法。针对低资源小语种，为了进一步提高语音识别任务性能，我们<strong>探索了多种微调策略：无标注音频数据利用、多语种微调和音素识别任务辅助。</strong>下图中分别展示了常规微调和三种策略的流程示意。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/fA3myeqiIr6r.png" alt="remote.png" title="remote.png"/></figure>



<p>由于预训练模型是多语种的，因而在一定程度上具有多语言的通用性，但对于特定目标语种的针对性有所不足。<strong>我们提出</strong>&nbsp;<strong>FT2</strong>&nbsp;<strong>策略以借助目标语种的语音数据将预训练模型迁移到目标语种中，使用的音频数据来自</strong>&nbsp;<strong>BABEL</strong>&nbsp;<strong>数据集。在第一阶段，我们先通过对比损失和码本多样性损失以自监督的方式微调整个模型。第二阶段，我们使用该语种的标注</strong>&nbsp;<strong>10h</strong>&nbsp;<strong>数据对第一阶段得到的模型进行</strong>&nbsp;<strong>CTC</strong>&nbsp;<strong>准则微调。两阶段微调方法有助于预训练模型有效地适应单一目标语种。</strong>&nbsp;下表中展示了传统混合系统和微调系统&nbsp;FT&nbsp;、两阶段微调系统&nbsp;FT2&nbsp;的语音识别结果，可以看到&nbsp;FT2&nbsp;系统借助额外无标注语音数据取得了明显性能提升。从平均值来看，我们提出的策略相比于传统最优混合单系统在词错误率上可以相对降低&nbsp;8.5%&nbsp;。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/iy2MJfqeQ3Mj.png" alt="remote.png" title="remote.png"/></figure>



<p>在另外两个策略上，<strong>我们通过实验验证了单语种微调相比于多语种微调的性能优越性，前者凭借更高的语种针对性在目标语种的语音识别任务上能够取得更优的性能，而后者则帮助模型学习更加通用鲁棒的表征。</strong>我们也借助实验说明了音素识别任务对于语音识别任务的辅助作用，且提升效果在低资源条件下更为明显。</p>



<p><strong>三、基于预训练模型的其他系统框架</strong></p>



<p>表征学习是机器学习中一种流行方法，不同表征往往可以揭示在数据背后隐藏的不同层面可解释因素。我们采用自监督预训练模型作为模型主干或用于提取表征，重点讨论低资源&nbsp;ASR&nbsp;任务中不同&nbsp;Transformer&nbsp;块的输出表征，系统框架包括&nbsp;CTC/Attention&nbsp;端到端系统和传统&nbsp;NN/HMM&nbsp;混合系统，如下图所示。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/fAzeUnzMzmm2.png" alt="remote.png" title="remote.png"/></figure>



<p><strong>3.1&nbsp;</strong><strong>端到端系统</strong><strong></strong></p>



<p>近期基于Transformer和Conformer的端到端ASR系统凭借其优异性能备受关注。但由于数据驱动的特性对低资源条件下的应用有所受限。借助预训练模型则可以应对标注数据资源不足带来的挑战，从而发挥端到端系统的性能优势。</p>



<p>将预训练模型应用于端到端系统有两种方法：第一种是把预训练模型作为编码器部分，在此基础上添加基于Transformer的解码器来构建完整的encoder-decoder系统；第二种则是将预训练模型作为前端模块来提取表征，将提取到的表征输入到端到端系统进行模型训练。上述两种方法分别记为为EN和FE。通过实验发现前者性能表现欠佳，因此主要采用后者进行后续探索。</p>



<p><strong>3.2 DNN/HMM</strong><strong>混合系统</strong><strong></strong></p>



<p>在传统混合系统中，我们依旧将预训练模型作为特征提取器。将音频数据输入到预训练模型中，从指定的&nbsp;Transformer&nbsp;层中提取表征，作为声学模型的特征进行使用。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0A/u2y2YrV7fEF3.png" alt="remote.png" title="remote.png"/></figure>



<p>实验使用波斯语数据集，为方便对比我们将不同系统使用不同预训练模型各层表征的语音识别性能情况展示在上图中。可以看到，<strong>从预训练模型的第</strong><strong>18</strong><strong>层到第</strong><strong>21</strong><strong>层</strong><strong>Transformer</strong><strong>提取表征对于</strong><strong>ASR</strong><strong>任务相对更</strong><strong>有利。</strong>微调后预训练模型XLSR-53-FT取得了最低的词错误率，同时<strong>对比微调后的模型</strong><strong>XLSR-53-FT</strong><strong>和原预训练模型</strong><strong>XLSR-53</strong><strong>，主要差异集中在最后</strong><strong>3</strong><strong>层，后者的最后</strong><strong>3</strong><strong>层表征在端到端和传统混合系统中均表现出明显的性能下降。</strong>从图中折线变化来看，WavLM和微调后的wav2vec2.0模型XLSR-53-FT趋势一致，而HuBERT则和XLSR-53更加接近。此外，<strong>相比于传统混合系统，端到端系统对来自不同预训练模型或同一预训练模型中不同层的表征更为敏感。</strong><strong></strong></p>



<p><strong>四、总结</strong></p>



<p>本文将wav2vec2.0，HuBERT和WavLM自监督预训练模型应用于低资源语种语音识别系统中，在10小时标注数据集的条件下对15个低资源小语种的识别性能进行提升。研究包括预训练模型使用的数据和体系架构、微调方法、预训练表征提取与分析、以及在端到端和混合系统中的进一步应用。</p>



<p>我们发现，<strong>目标语种的音频数据对于预训练模型的迁移应用具有明显帮助作用，借助自监督训练进行模型微调可以使得预训练模型更好地适应目标语种。多语言预训练模型在跨语种语音识别中具有优势，但多语种微调相比于单语种微调并未取得性能提升。我们通过实验验证了音素识别任务可以提高低资源语音识别的性能。</strong></p>



<p>同时我们也<strong>深入分析了预训练模型各个</strong><strong>Transformer</strong><strong>层输出表征之间的关系，了解不同语种、不同预训练模型、不同微调方法带来的变化。并进一步在端到端和传统混合系统中对预训练表征进行了对比和应用。</strong></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>学术报告：咸鱼也要有梦——从计算机视觉到AI4Science的探索</title>
		<link>/2023/05/16/%e5%ad%a6%e6%9c%af%e6%8a%a5%e5%91%8a%ef%bc%9a%e5%92%b8%e9%b1%bc%e4%b9%9f%e8%a6%81%e6%9c%89%e6%a2%a6-%e4%bb%8e%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89%e5%88%b0ai4science%e7%9a%84/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Tue, 16 May 2023 05:54:22 +0000</pubDate>
				<category><![CDATA[research]]></category>
		<guid isPermaLink="false">/?p=120</guid>

					<description><![CDATA[报告人：欧阳万里，浦江实验室领军科学家 时间：2022/12/11 [周日]&#160; 上午 11:00-1 ... <a title="学术报告：咸鱼也要有梦——从计算机视觉到AI4Science的探索" class="read-more" href="/2023/05/16/%e5%ad%a6%e6%9c%af%e6%8a%a5%e5%91%8a%ef%bc%9a%e5%92%b8%e9%b1%bc%e4%b9%9f%e8%a6%81%e6%9c%89%e6%a2%a6-%e4%bb%8e%e8%ae%a1%e7%ae%97%e6%9c%ba%e8%a7%86%e8%a7%89%e5%88%b0ai4science%e7%9a%84/" aria-label="继续阅读学术报告：咸鱼也要有梦——从计算机视觉到AI4Science的探索">阅读更多</a>]]></description>
										<content:encoded><![CDATA[<div class="wp-block-image">
<figure class="aligncenter size-large is-resized"><img fetchpriority="high" decoding="async" src="/wp-content/uploads/2023/05/image-972x1024.png" alt="" class="wp-image-121" width="243" height="256" srcset="/wp-content/uploads/2023/05/image-972x1024.png 972w, /wp-content/uploads/2023/05/image-285x300.png 285w, /wp-content/uploads/2023/05/image-768x809.png 768w, /wp-content/uploads/2023/05/image-1458x1536.png 1458w, /wp-content/uploads/2023/05/image.png 1519w" sizes="(max-width: 243px) 100vw, 243px" /></figure></div>


<p>报告人：欧阳万里，浦江实验室领军科学家</p>



<p>时间：2022/12/11 [周日]&nbsp; 上午 11:00-12:00</p>



<p>地点：腾讯会议 703-938-233</p>



<p>题目：咸鱼也要有梦——从计算机视觉到AI4Science的探索</p>



<p>摘要：本次报告将结合报告人的学术经历介绍尽管不可能每个人都是英雄，但仍可怀抱梦想，做出让自己快乐的科研工作。具体而言，将从报告人在计算机视觉领域的科研成果及其转型做AI4Science的动机出发，详细介绍当前其在实验室进行的A14Science领域的工作计划。同时，也将介绍目前最前沿的Science研究，包括今年的诺贝尔物理学奖和化学奖，简述量子信息技术，点击化学和生物正交化学概念及其应用，以及人工智能将在加速科学问题研究中扮演何等重要的角色。</p>



<p>报告人简介：欧阳万里在香港中文大学获得博士学位，是深度学习与计算机视觉领域的专家，浦江实验室领军科学家，曾任悉尼大学电子信息工程学院研究主任。他主要从事计算机视觉和AI for Sience的研究。他的学术成果卓越，截至目前，谷歌学术引用达30000+，H-index指数达70+。据AIMiner报导，在计算机视觉领域影响力为全球第50、澳大利亚第2。获得悉尼大学杰出科研校长奖（Vice Chancellor&#8217;s award for &#8220;Outstanding Research&#8221;，同年悉尼大学工程学院仅2人获此奖）。据国际权威机构爱斯维尔对计算机视觉与模式识别领域2016-2021发表论文的统计，平均每篇文章引用次数(Citations per paper) 和领域加权影响力（field-weighted citation impact, FWC）皆为澳大利亚第1。两篇文章入选paperdigest CVPR/CCV最有影响力的文章。担任ICCV最佳审稿人，IJCV和Pattern Recognition编委，TPAM客座编辑，CVPR2023资深领域主席，ICCV2019展示主席，CVPR2021、ICCV2021领域主席。</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>利用BERT提高语种识别性能</title>
		<link>/2023/05/16/%e5%88%a9%e7%94%a8bert%e6%8f%90%e9%ab%98%e8%af%ad%e7%a7%8d%e8%af%86%e5%88%ab%e6%80%a7%e8%83%bd/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Tue, 16 May 2023 05:52:49 +0000</pubDate>
				<category><![CDATA[news]]></category>
		<guid isPermaLink="false">/?p=118</guid>

					<description><![CDATA[本文介绍清华大学语音与音频技术实验室（SATLab）ISCSLP 2022录用论文。BERT-LID: Lev ... <a title="利用BERT提高语种识别性能" class="read-more" href="/2023/05/16/%e5%88%a9%e7%94%a8bert%e6%8f%90%e9%ab%98%e8%af%ad%e7%a7%8d%e8%af%86%e5%88%ab%e6%80%a7%e8%83%bd/" aria-label="继续阅读利用BERT提高语种识别性能">阅读更多</a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/qm67vea2EBne.png" alt=""/></figure>



<p>本文介绍清华大学语音与音频技术实验室（SATLab）ISCSLP 2022录用论文。BERT-LID: Leveraging BERT to Improve Spoken Language Identification。这篇文章将BERT模型引入到语种识别领域。利用BERT模型的优越性，再结合下游不同的神经网络模型，提升语种识别能力，尤其是在短语音的情况下识别性能有更为明显提升。</p>



<blockquote class="wp-block-quote">
<p></p>
</blockquote>



<h1 class="wp-block-heading"><strong>01 语种识别</strong></h1>



<p>语种识别是分析处理语音片段以判断其所属的语种。&nbsp;它&nbsp;对智能语音系统中的多语言模块有着深远的影响。目前语种&nbsp;识别技术在中长语音（&gt;3s）上能够实现较高的准确率&nbsp;，但是在短语音（&lt;=1s）上&nbsp;的表现并不能令人满意。由于短语音提供的数据信息较少，因此大大增加了识别难度。</p>



<p>我们尝试将在自然语言处理中表现甚好的BERT模型应用到语种识别任务中，旨在提升短语音情况下语种识别模型的性能。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/7jEjAzEBNNfa.jpg" alt="640.jpg" title="640.jpg"/></figure>



<p><em>语种识别（图片来自第五届东方语种识别竞赛）</em></p>



<h1 class="wp-block-heading"><strong>02 BERT-LID</strong></h1>



<p>BERT的全称是：Bidirectional Encoder Representation from Transformers。该模型通过大规模无标注文本语料训练，获得包含丰富语义信息的表征，BERT具有<strong>强大的语义理解能力</strong>，是近年来自然语言处理领域公认的里程碑模型。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/R3Q3If7BJzyy.png" alt="640.png" title="640.png"/></figure>



<p><em>BERT模型（图片来自</em><a href="https://arxiv.org/pdf/1810.04805.pdf%EF%BC%89"><em>https://arxiv.org/pdf/1810.04805.pdf）</em></a></p>



<p>原始的BERT旨在处理基于文本的表示，而我们这里的输入是语音，因此需要对输入语音进行预处理以适应该模型。</p>



<p>方法一：提取语音的音素，作为BERT模型的输入。</p>



<p>方法二：提取语音的后验概率特征，替换原始的Token Embedding结果，作为输入。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/nUJnYv2uUzIr.jpg" alt="640-_1_.jpg" title="640-_1_.jpg"/></figure>



<p><em>将调整过后的BERT接不同的分类网络模型进行学习</em></p>



<p>我们通过对整体网络进行训练以获得最佳BERT-LID模型。</p>



<h1 class="wp-block-heading"><strong>03 实验及结论</strong></h1>



<p>我们使用OLR20、TAL_ASR、TIMIT和THCHS30数据集来对BERT模型进行训练。对于音频数据我们使用BUT的开源代码来获得音素序列以及音素后验概率特征。</p>



<p>其中OLR20来自于2020年东方语种识别竞赛所提供的数据，包含6个语种；TAL_ASR为好未来英语课授课音频，每条音频只有一位说话人，包含中英文混合讲话的情况（对于这种情况，我们使用强制对齐的方法来获得中文与英文的标签信息）；TIMIT为英文数据集；THCHS30为中文数据集。同时我们还对数据进行切分处理来获得时长为1s的短语音数据。数据集的具体情况如下图所示，其中T&amp;T为TIMIT和THCHS30的切分短语音混合使用的情况。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/VRZvEveA7Jfe.png" alt="640 (1).png" title="remote.png"/></figure>



<p><em>数据集</em></p>



<p>下游神经网络模块，我们选择使用CNN、LSTM、RCNN、DPCNN分别进行实验。首先我们在OLR20和T&amp;T数据上对BERT-LID模型进行测试，然后进行消融实验：a）BERT部分结合线性分类层来得到结果（称之为BERT）；b）去掉BERT模块（称之为LID），直接将数据输入到对应的模型中来得到结果。可以看出，相比于BERT以及LID模块，<strong>BERT-LID模型在语种识别任务中准确率整体上有所提升</strong>。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/fmeIBru2yaQf.png" alt="640 (2).png" title="remote.png"/></figure>



<p><em>在OLR20和T&amp;T数据集上进行消融实验</em></p>



<p>之后，我们在BERT-RCNN、x-vextor、n-gram-svm模型上进行对比实验（TAL_ASR和T&amp;T数据集为短音频数据的集合），其中x-vextor、n-gram-svm为我们的基线系统。可以看出，BERT-LID模型在我们的不同数据集中都能有最优表现，尤其是<strong>在短语音情况下，我们所提出的方法有更为明显的提升</strong>。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/VrYZ3aAVzQny.png" alt="640 (3).png" title="remote.png"/></figure>



<p><em>在不同数据集上进行对比试验</em></p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>探索语音自监督模型的高效融合算法</title>
		<link>/2023/05/16/%e6%8e%a2%e7%b4%a2%e8%af%ad%e9%9f%b3%e8%87%aa%e7%9b%91%e7%9d%a3%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%ab%98%e6%95%88%e8%9e%8d%e5%90%88%e7%ae%97%e6%b3%95/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Tue, 16 May 2023 05:52:15 +0000</pubDate>
				<category><![CDATA[news]]></category>
		<guid isPermaLink="false">/?p=116</guid>

					<description><![CDATA[本文介绍了清华大学语音与音频技术实验室（SATLab）与上海交通大学跨媒体语言智能实验室（X-LANCE）合作 ... <a title="探索语音自监督模型的高效融合算法" class="read-more" href="/2023/05/16/%e6%8e%a2%e7%b4%a2%e8%af%ad%e9%9f%b3%e8%87%aa%e7%9b%91%e7%9d%a3%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%ab%98%e6%95%88%e8%9e%8d%e5%90%88%e7%ae%97%e6%b3%95/" aria-label="继续阅读探索语音自监督模型的高效融合算法">阅读更多</a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image is-resized"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/qeuqUzFVzIZz.png" alt="" width="128" height="128"/></figure>



<p>本文介绍了清华大学语音与音频技术实验室（SATLab）与上海交通大学跨媒体语言智能实验室（X-LANCE）合作的NCMMSC录用论文：Exploring Effective Fusion Algorithms for Speech Based Self-Supervised Learning Models。该论文提出了一系列语音自监督模型融合算法，并基于语音自监督模型的评测框架SUPERB展开一系列实验。实验结果表明，该论文中提出的融合算法，有效地结合了目前表现出色的语音自监督模型 HuBERT 与 Data2vec 的优势，提高了模型在说话人识别与语音识别任务上的表现。</p>



<h1 class="wp-block-heading"><strong>一&nbsp;语音自监督模型各有偏好</strong><strong></strong></h1>



<p>近年来，自监督学习在语音领域取得巨大成功。语音自监督学习的一般思想，是基于语音的上下文信息进行重构或预测自身，使模型能够在无监督的情况下有效地学习底层结构信息。语音自监督模型可以在大量的无标记语音数据上进行预训练，然后在特定的下游任务上用少量的标注语音进行微调，以实现显著的性能提高。目前已涌现出一系列成功的语音自监督模型，如Wav2vec 2.0，HuBERT，WavLM，Data2vec等。</p>



<p>现有的研究表明，不同的语音自监督模型，对下游任务的偏好不同。在我们的工作中，我们基于SUPERB——一个语音自监督模型评测框架，首次评测了&nbsp;Data2vec&nbsp;在不同任务上的表现，并与现有的自监督模型进行对比。我们发现，一些模型在说话人相关的任务上表现出色，例如&nbsp;HuBERT；一些模型在内容相关的任务上出类拔萃，例如&nbsp;Data2vec。同时我们也发现，对于偏好不同的模型，其内部的transformer层编码了截然不同的信息，如图1所示。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/BFbY7rIRfYba.png" alt="640 (4).png" title="640 (4).png"/></figure>



<p>图1.&nbsp;SUPERB评测系统中，HuBERT&nbsp;与&nbsp;Data2vec&nbsp;在不同下游任务上的权重对比</p>



<p>我们思考，能否做到一种融合算法，将不同自监督模型的信息有效地提取并融合，使融合模型能够综合各模型的优点，在不同的下游任务上做到“十项全能”呢？基于此，我们提出了一系列融合算法，希望能够融合不同模型的优势。</p>



<h1 class="wp-block-heading">二 语音自监督模型融合算法</h1>



<p>我们提出并比较了四种针对多种自监督模型的融合方法（如图2所示）：两种特征级融合和两种概率级融合。这四种方法的模型融合阶段，按照信息交互时间依次向后伸。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/meMvIv3M3Ibm.png" alt="640 (5).png" title="remote.png"/></figure>



<p>图2.&nbsp;语音自监督模型的四种融合方式</p>



<p>设函数F表示下游模型，m为要融合的模型个数，l为每个模型的层数，wij，hij分别表示第&nbsp;i&nbsp;个模型的第&nbsp;j&nbsp;层特征的权值和隐层向量。</p>



<p>第一种融合算法，是简单地直接融合各模型特征。如图（2）a&nbsp;图所示，我们直接将各模型各层特征进行线性加权，得到的融合特征送入下游模型中。这里，不同模型在前向传播后直接进行信息交流，最终针对特定任务的概率分布为</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/Y36f2i6ZFN7f.png" alt="640 (6).png" title="640 (6).png"/></figure>



<p>第二种融合算法，是对各模型进行结构化的融合。如图（2）b&nbsp;图所示，首先，我们对每个自监督模型的不同层的特征进行加权求和；然后，我们应用第一步中这些输出再进行结构化的加权，以得到下游模型的输入。这里，不同模型提取的信息经历了各自的特征融合器之后才进行交流融合，最终得到针对特定任务的概率分布为</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/MJ7vimee6jIr.png" alt="640 (7).png" title="640 (7).png"/></figure>



<p>第三种融合算法，是在概率层面进行融合。如图（2）c&nbsp;图所示，对于每个自监督模型，我们对不同层的特征进行加权，并将结果输入下游模型。下游模型的输出形成了一个任务标签的概率分布。我们在这里融合不同模型得到的概率分布，并使用融合的概率分布进行推理。不同模型提取的信息经过相同的下游模型后相互融合，最终得到的概率分布可表示为</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/RZ3YrmNBRfye.png" alt="640 (8).png" title="640 (8).png"/></figure>



<p>第四种融合算法，与第三种类似，但不同自监督模型各自享有独立的下游模型，如图（2）d&nbsp;图所示。不同下游模型产生的概率分布将被融合，融合的分布将用于最终的推断，最终的概率分布可表示为</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/IRrUJzQFFVbq.png" alt="640 (9).png" title="640 (9).png"/></figure>



<h1 class="wp-block-heading">三 模型融合实验与分析</h1>



<p>为了更好地对比模型融合的效果，我们基于SUPERB，令自监督模型在微调过程中被冻结，只有较小的下游模型与模型的特征融合器（Featurizer）可以被更新。我们选择&nbsp;Data2vec&nbsp;和&nbsp;HuBERT&nbsp;作为待融合的模型，因为他们对下游任务不同的偏好，正是我们想要的。我们主要分析两大任务：说话人识别与语音识别，它们分别是说话人相关任务与内容相关任务的典型代表。</p>



<p>如图3所示，实验结果表明，对于说话人相关任务，仅仅是简单的特征融合，会大幅降低识别的准确率。这一大幅衰减，可以通过结构化的模型融合来避免。这一结论同样对语音识别任务有效。这可能是因为，如果直接融合往往会混淆信息，因为不同模型的内在特征有很大的不同，进行结构化加权后能够更好地利用模型的能力，而不会引起模型间的信息混淆。</p>



<p>同时，我们发现，对于说话人识别任务，信息交换的阶段越接近任务的标签，融合效果就越加有效。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/fARziyMbErum.png" alt="640 (10).png" title="640 (10).png"/></figure>



<p>图3. 四种融合方式在 SID 任务与 ASR&nbsp;任务的表现</p>



<p>在大模型上，结构化融合同样适用。我们尝试了语音识别任务上大模型的结构化融合，词错率降低了7%，目前仍然是SUPERB上的最佳结果。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/bi6RVvmmQNvi.png" alt="640 (11).png" title="640 (11).png"/></figure>



<p>图4.&nbsp;结构化融合有利于大模型语音识别</p>



<p>我们提出了一系列模型融合方法，旨在综合不同语音自监督模型的优势。实验结果表明，我们提出的方法有效地综合了不同语音自监督模型在不同任务上的能力，相对于单个模型而言，融合模型的能力取得了显著的提升。</p>



<p><strong>第一作者简介</strong></p>



<figure class="wp-block-image is-resized"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/qmUreyInaeia.jpg" alt="640-_2_.jpg" width="200" height="302" title="640-_2_.jpg"/></figure>



<p>唐昌礼，清华大学电子工程系2020级本科生，曾于清华大学语音与音频技术实验室、上海交通大学跨媒体语言智能实验室参与 SRT 项目。</p>



<figure class="wp-block-image is-resized"><img loading="lazy" decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/22AzEbMjaaYf.jpg" alt="640-_3_.jpg" width="270" height="407" title="640-_3_.jpg"/></figure>



<p>王与进，清华大学电子工程系2020级本科生，曾于清华大学语音与音频技术实验室、上海交通大学跨媒体语言智能实验室参与&nbsp;SRT 项目。</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>基于声学模型共享的零资源韩语识别系统</title>
		<link>/2023/05/16/%e5%9f%ba%e4%ba%8e%e5%a3%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b%e5%85%b1%e4%ba%ab%e7%9a%84%e9%9b%b6%e8%b5%84%e6%ba%90%e9%9f%a9%e8%af%ad%e8%af%86%e5%88%ab%e7%b3%bb%e7%bb%9f/</link>
		
		<dc:creator><![CDATA[root1]]></dc:creator>
		<pubDate>Tue, 16 May 2023 05:49:31 +0000</pubDate>
				<category><![CDATA[news]]></category>
		<guid isPermaLink="false">/?p=114</guid>

					<description><![CDATA[声学模型共享方法是极低资源小语种语音识别一种解决方案，能够实现不需要任何语音数据的语音识别。本文介绍清华大学语 ... <a title="基于声学模型共享的零资源韩语识别系统" class="read-more" href="/2023/05/16/%e5%9f%ba%e4%ba%8e%e5%a3%b0%e5%ad%a6%e6%a8%a1%e5%9e%8b%e5%85%b1%e4%ba%ab%e7%9a%84%e9%9b%b6%e8%b5%84%e6%ba%90%e9%9f%a9%e8%af%ad%e8%af%86%e5%88%ab%e7%b3%bb%e7%bb%9f/" aria-label="继续阅读基于声学模型共享的零资源韩语识别系统">阅读更多</a>]]></description>
										<content:encoded><![CDATA[
<figure class="wp-block-image size-full is-resized"><img loading="lazy" decoding="async" src="/wp-content/uploads/2023/05/V7zyIvZ7NBR3.jpg" alt="" class="wp-image-16" width="256" height="128" srcset="/wp-content/uploads/2023/05/V7zyIvZ7NBR3.jpg 512w, /wp-content/uploads/2023/05/V7zyIvZ7NBR3-300x150.jpg 300w" sizes="(max-width: 256px) 100vw, 256px" /></figure>



<p>声学模型共享方法是极低资源小语种语音识别一种解决方案，能够实现不需要任何语音数据的语音识别。本文介绍清华大学语音与音频技术实验室的零资源韩语语音系统，其在不使用任何韩语语音数据的情况下，在Zeroth韩语数据集上的测试CER达到了27.33%。</p>



<h1 class="wp-block-heading">一&nbsp;<strong>声学模型共享与零资源ASR</strong></h1>



<p>小语种语音识别一直是语音领域值得关注的问题之一，几千种小语种普遍面临着训练数据不足、收集训练数据困难等问题，而声学模型共享方法则可以实现<strong>不需训练数据的语音识别</strong>，从而为这一问题提供了一个方向。这一方法利用语种之间的相似性，直接使用常见语言的声学模型，结合低资源语言的语言模型、发音字典以及两种语言之间的音素映射关系等专家知识，就可以构建较为精准的语音识别系统。</p>



<p>我们将声学模型共享方法扩展到了零资源韩语语音识别上。我们使用Kaldi工具包，利用汉语训练声学模型，根据汉语和韩语两种语言之间的音素相似性设置了两种不同的音素映射方案，并比较了不同方案的优劣。实验结果表明，我们的系统可以在<strong>不使用任何韩语训练数据的情况下达到27.33%的CER</strong>。</p>



<h1 class="wp-block-heading">二&nbsp;<strong>韩语的声学模型共享</strong></h1>



<p>在书写上，韩语是一种表音文字，其书写体系中的符号与音素存在着紧密的对应关系。在发音上，韩语是一种音节语言，一个韩语音节由一个元音（中声），元音前的一个可选的辅音（初声）和元音后的一个可选的辅音（终声）构成。韩语包含19个辅音和21个元音。在韩语中，根据上下文的不同，音素可能被替换、删除或是添加，两个相邻的音素可能会发生合并，因此即使韩语是完全的表音文字，一个句子的字面内容和其发音仍可能存在不同。</p>



<p>为了实现声学模型共享，我们需要建立韩语和汉语之间的音素对应关系。一种方法是将汉语词用韩语音素表示(zh2kr)。这种方法在训练过程中就引入音素对应关系，训练集中的汉语被转写为相近的韩语音素，而得到的模型可以被视为一个用<strong>汉语语音学习得到的韩语语音识别模型</strong>。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/ZNRNjyiQfMJ3.png" alt="640 (12).png" title="640 (12).png"/></figure>



<p>汉语音素到韩语音素的对应关系（部分）</p>



<p>另一种方法是将韩语词用汉语音素表示(kr2zh)。这种方法是在声学模型训练完成后引入音素对应关系。<strong>通过修改发音词典，将韩语词统一表示为相近的汉语音素</strong>，使用汉语正常训练的声学模型就可以用来识别韩语。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/32A7Jbe2uyEj.png" alt="640 (13).png" title="640 (13).png"/></figure>



<p>韩语音素到汉语音素的对应关系（部分）</p>



<h1 class="wp-block-heading">三&nbsp;<strong>实验设置与结果</strong></h1>



<p>我们使用Aishell1数据集训练汉语声学模型，测试集则选用Zeroth开源韩语数据集的测试集。声学模型结构方面，我们使用了<strong>11层TDNN</strong>，输入为<strong>40维MFCC</strong>特征；语言模型方面，我们使用Zeroth训练集文本训练了3-gram语言模型。我们的基线系统是使用Zeroth的90小时韩语数据训练得到的相同结构的TDNN模型。</p>



<p>实验结果表明，尽管与使用充足有标注数据训练的ASR模型仍有较大差距，<strong>我们的零资源语音识别模型仍能实现较低的错误率</strong>。另外，相比kr2zh方法，zh2kr方法的精度有大幅度的下降。</p>



<p>我们认为，这是由于zh2kr方法需要为汉语中存在而韩语中不存在的音素指定近似的对应关系，这使得模型学习到的韩语音素对应的汉语声学特征与测试集中真正的韩语声学特征的分布有较大差异，这些<strong>人工引入的额外的领域漂移</strong>影响了最终的识别效果。</p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/FZNn2iuIfmea.png" alt="640 (14).png" title="640 (14).png"/></figure>



<p>不同映射方法与有监督方法的比较。零资源方法能达到较低的CER，而kr2zh方法要优于zh2kr方法</p>



<p>我们的方法将无监督预训练模型应用于零资源语音识别任务，在不使用目标语种的任何语音数据的情况下实现了平均33%的WER。在无训练数据或可获得的训练数据小于10小时的情况下，我们的零资源方法相比有监督方法有较大优势。</p>



<p><strong>作者简介</strong></p>



<figure class="wp-block-image"><img decoding="async" src="http://web.ee.tsinghua.edu.cn/_tsf/00/0B/jE7rYz6bqIJz.jpg" alt="640-_4_.jpg" title="640-_4_.jpg"/></figure>



<p>王皓宇，清华大学电子工程系语音与音频技术实验室研究生二年级学生，主要研究方向为低资源语音识别和预训练模型蒸馏。</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
